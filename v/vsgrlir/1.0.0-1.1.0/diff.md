# Comparing `tmp/vsgrlir-1.0.0-py3-none-any.whl.zip` & `tmp/vsgrlir-1.1.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,15 +1,30 @@
-Zip file size: 31147 bytes, number of entries: 13
--rw-r--r--  2.0 fat     6928 b- defN 20-Feb-02 00:00 vsgrlir/__init__.py
--rw-r--r--  2.0 fat      779 b- defN 20-Feb-02 00:00 vsgrlir/__main__.py
--rw-r--r--  2.0 fat    25488 b- defN 20-Feb-02 00:00 vsgrlir/grl.py
--rw-r--r--  2.0 fat    40304 b- defN 20-Feb-02 00:00 vsgrlir/mixed_attn_block.py
--rw-r--r--  2.0 fat    20293 b- defN 20-Feb-02 00:00 vsgrlir/mixed_attn_block_efficient.py
--rw-r--r--  2.0 fat    17254 b- defN 20-Feb-02 00:00 vsgrlir/ops.py
--rw-r--r--  2.0 fat    21259 b- defN 20-Feb-02 00:00 vsgrlir/swin_v1_block.py
+Zip file size: 36384 bytes, number of entries: 28
+-rw-r--r--  2.0 fat    22319 b- defN 20-Feb-02 00:00 vsgrlir/__init__.py
+-rw-r--r--  2.0 fat     1379 b- defN 20-Feb-02 00:00 vsgrlir/__main__.py
+-rw-r--r--  2.0 fat    32347 b- defN 20-Feb-02 00:00 vsgrlir/grl.py
+-rw-r--r--  2.0 fat    40252 b- defN 20-Feb-02 00:00 vsgrlir/mixed_attn_block.py
+-rw-r--r--  2.0 fat    20448 b- defN 20-Feb-02 00:00 vsgrlir/mixed_attn_block_efficient.py
+-rw-r--r--  2.0 fat    17487 b- defN 20-Feb-02 00:00 vsgrlir/ops.py
+-rw-r--r--  2.0 fat    21228 b- defN 20-Feb-02 00:00 vsgrlir/swin_v1_block.py
 -rw-r--r--  2.0 fat     1660 b- defN 20-Feb-02 00:00 vsgrlir/upsample.py
 -rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/bsr_grl_base.ckpt
-?rw-r--r--  2.0 fat     2557 b- defN 20-Feb-02 00:00 vsgrlir-1.0.0.dist-info/METADATA
-?rw-r--r--  2.0 fat       87 b- defN 20-Feb-02 00:00 vsgrlir-1.0.0.dist-info/WHEEL
-?rw-r--r--  2.0 fat     1084 b- defN 20-Feb-02 00:00 vsgrlir-1.0.0.dist-info/licenses/LICENSE
-?rw-r--r--  2.0 fat     1022 b- defN 20-Feb-02 00:00 vsgrlir-1.0.0.dist-info/RECORD
-13 files, 138715 bytes uncompressed, 29465 bytes compressed:  78.8%
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/db_defocus_single_pixel_grl_base.ckpt
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/db_motion_grl_base_gopro.ckpt
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/db_motion_grl_base_realblur_j.ckpt
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/db_motion_grl_base_realblur_r.ckpt
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/dm_grl_small.ckpt
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/dn_grl_small_c3s15.ckpt
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/dn_grl_small_c3s25.ckpt
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/dn_grl_small_c3s50.ckpt
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/jpeg_grl_small_c3q10.ckpt
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/jpeg_grl_small_c3q20.ckpt
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/jpeg_grl_small_c3q30.ckpt
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/jpeg_grl_small_c3q40.ckpt
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/sr_grl_small_c3x2.ckpt
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/sr_grl_small_c3x3.ckpt
+-rw-r--r--  2.0 fat        0 b- defN 20-Feb-02 00:00 vsgrlir/models/sr_grl_small_c3x4.ckpt
+?rw-r--r--  2.0 fat     2550 b- defN 20-Feb-02 00:00 vsgrlir-1.1.0.dist-info/METADATA
+?rw-r--r--  2.0 fat       87 b- defN 20-Feb-02 00:00 vsgrlir-1.1.0.dist-info/WHEEL
+?rw-r--r--  2.0 fat     1084 b- defN 20-Feb-02 00:00 vsgrlir-1.1.0.dist-info/licenses/LICENSE
+?rw-r--r--  2.0 fat     2445 b- defN 20-Feb-02 00:00 vsgrlir-1.1.0.dist-info/RECORD
+28 files, 163286 bytes uncompressed, 32340 bytes compressed:  80.2%
```

## zipnote {}

```diff
@@ -21,20 +21,65 @@
 
 Filename: vsgrlir/upsample.py
 Comment: 
 
 Filename: vsgrlir/models/bsr_grl_base.ckpt
 Comment: 
 
-Filename: vsgrlir-1.0.0.dist-info/METADATA
+Filename: vsgrlir/models/db_defocus_single_pixel_grl_base.ckpt
 Comment: 
 
-Filename: vsgrlir-1.0.0.dist-info/WHEEL
+Filename: vsgrlir/models/db_motion_grl_base_gopro.ckpt
 Comment: 
 
-Filename: vsgrlir-1.0.0.dist-info/licenses/LICENSE
+Filename: vsgrlir/models/db_motion_grl_base_realblur_j.ckpt
 Comment: 
 
-Filename: vsgrlir-1.0.0.dist-info/RECORD
+Filename: vsgrlir/models/db_motion_grl_base_realblur_r.ckpt
+Comment: 
+
+Filename: vsgrlir/models/dm_grl_small.ckpt
+Comment: 
+
+Filename: vsgrlir/models/dn_grl_small_c3s15.ckpt
+Comment: 
+
+Filename: vsgrlir/models/dn_grl_small_c3s25.ckpt
+Comment: 
+
+Filename: vsgrlir/models/dn_grl_small_c3s50.ckpt
+Comment: 
+
+Filename: vsgrlir/models/jpeg_grl_small_c3q10.ckpt
+Comment: 
+
+Filename: vsgrlir/models/jpeg_grl_small_c3q20.ckpt
+Comment: 
+
+Filename: vsgrlir/models/jpeg_grl_small_c3q30.ckpt
+Comment: 
+
+Filename: vsgrlir/models/jpeg_grl_small_c3q40.ckpt
+Comment: 
+
+Filename: vsgrlir/models/sr_grl_small_c3x2.ckpt
+Comment: 
+
+Filename: vsgrlir/models/sr_grl_small_c3x3.ckpt
+Comment: 
+
+Filename: vsgrlir/models/sr_grl_small_c3x4.ckpt
+Comment: 
+
+Filename: vsgrlir-1.1.0.dist-info/METADATA
+Comment: 
+
+Filename: vsgrlir-1.1.0.dist-info/WHEEL
+Comment: 
+
+Filename: vsgrlir-1.1.0.dist-info/licenses/LICENSE
+Comment: 
+
+Filename: vsgrlir-1.1.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## vsgrlir/__init__.py

```diff
@@ -1,95 +1,469 @@
 from __future__ import annotations
 
 import math
 import os
+from dataclasses import dataclass
 from threading import Lock
 
 import numpy as np
 import torch
+import torch.nn.functional as F
 import vapoursynth as vs
+from vsutil import fallback
 
 from .grl import GRL
 
-__version__ = "1.0.0"
+__version__ = "1.1.0"
 
 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
 
 model_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "models")
 
 
+class Backend:
+    @dataclass
+    class Eager:
+        module: torch.nn.Module
+
+    @dataclass
+    class CUDAGraphs:
+        graph: list[torch.cuda.CUDAGraph]
+        static_input: list[torch.Tensor]
+        static_output: list[torch.Tensor]
+
+
 @torch.inference_mode()
 def grlir(
     clip: vs.VideoNode,
     device_index: int | None = None,
     num_streams: int = 1,
+    cuda_graphs: bool = False,
+    model: int = 0,
     tile_w: int = 0,
     tile_h: int = 0,
-    tile_pad: int = 16,
+    tile_pad: int | None = None,
 ) -> vs.VideoNode:
     """Efficient and Explicit Modelling of Image Hierarchies for Image Restoration
 
-    :param clip:            Clip to process. Only RGBS format is supported.
+    :param clip:            Clip to process. Only RGBH and RGBS formats are supported.
+                            RGBH performs inference in FP16 mode while RGBS performs inference in FP32 mode.
     :param device_index:    Device ordinal of the GPU.
     :param num_streams:     Number of CUDA streams to enqueue the kernels.
+    :param cuda_graphs:     Use CUDA Graphs to remove CPU overhead associated with launching CUDA kernels sequentially.
+    :param model:           Model to use.
+                             0 = Blind Image SR
+                             1 = Defocus Deblurring
+                             2 = Motion Deblurring (GoPro)
+                             3 = Motion Deblurring (RealBlur-J)
+                             4 = Motion Deblurring (RealBlur-R)
+                             5 = Demosaicking
+                             6 = Denoising (sigma 15)
+                             7 = Denoising (sigma 25)
+                             8 = Denoising (sigma 50)
+                             9 = JPEG compression artifact removal (quality 10)
+                            10 = JPEG compression artifact removal (quality 20)
+                            11 = JPEG compression artifact removal (quality 30)
+                            12 = JPEG compression artifact removal (quality 40)
+                            13 = Classical Image SR (scale 2)
+                            14 = Classical Image SR (scale 3)
+                            15 = Classical Image SR (scale 4)
     :param tile_w:          Tile width. As too large images result in the out of GPU memory issue, so this tile option
                             will first crop input images into tiles, and then process each of them. Finally, they will
                             be merged into one image. 0 denotes for do not use tile.
     :param tile_h:          Tile height.
     :param tile_pad:        Pad size for each tile, to remove border artifacts.
     """
     if not isinstance(clip, vs.VideoNode):
         raise vs.Error("grlir: this is not a clip")
 
-    if clip.format.id != vs.RGBS:
-        raise vs.Error("grlir: only RGBS format is supported")
+    if clip.format.id not in [vs.RGBH, vs.RGBS]:
+        raise vs.Error("grlir: only RGBH and RGBS formats are supported")
 
     if not torch.cuda.is_available():
         raise vs.Error("grlir: CUDA is not available")
 
     if num_streams < 1:
         raise vs.Error("grlir: num_streams must be at least 1")
 
     if num_streams > vs.core.num_threads:
         raise vs.Error("grlir: setting num_streams greater than `core.num_threads` is useless")
 
+    if model not in range(16):
+        raise vs.Error("grlir: model must be between 0 and 15 (inclusive)")
+
     if os.path.getsize(os.path.join(model_dir, "bsr_grl_base.ckpt")) == 0:
         raise vs.Error("grlir: model files have not been downloaded. run 'python -m vsgrlir' first")
 
     torch.set_float32_matmul_precision("high")
 
+    fp16 = clip.format.bits_per_sample == 16
+    dtype = torch.half if fp16 else torch.float
+
     device = torch.device("cuda", device_index)
 
     stream = [torch.cuda.Stream(device=device) for _ in range(num_streams)]
     stream_lock = [Lock() for _ in range(num_streams)]
 
-    module = GRL(
-        upscale=4,
-        embed_dim=180,
-        img_size=128,
-        upsampler="nearest+conv",
-        depths=[4, 4, 8, 8, 8, 4, 4],
-        num_heads_window=[3, 3, 3, 3, 3, 3, 3],
-        num_heads_stripe=[3, 3, 3, 3, 3, 3, 3],
-        window_size=16,
-        stripe_size=[32, 64],
-        stripe_shift=True,
-        mlp_ratio=2,
-        anchor_window_down_factor=4,
-        local_connection=True,
-    )
-    scale = 4
-
-    model_path = os.path.join(model_dir, "bsr_grl_base.ckpt")
-
-    state_dict = torch.load(model_path, map_location="cpu")["state_dict"]
-    state_dict = {k.replace("model_g.", ""): v for k, v in state_dict.items() if "model_g." in k}
+    match model:
+        case 0:
+            model_name = "bsr_grl_base.ckpt"
+            model_args = dict(
+                embed_dim=180,
+                upscale=4,
+                upsampler="nearest+conv",
+                depths=[4, 4, 8, 8, 8, 4, 4],
+                num_heads_window=[3, 3, 3, 3, 3, 3, 3],
+                num_heads_stripe=[3, 3, 3, 3, 3, 3, 3],
+                window_size=16,
+                stripe_size=[32, 64],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=True,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 16)
+            pad_size = 64
+        case 1:
+            model_name = "db_defocus_single_pixel_grl_base.ckpt"
+            model_args = dict(
+                embed_dim=180,
+                upscale=1,
+                upsampler="",
+                depths=[4, 4, 8, 8, 8, 4, 4],
+                num_heads_window=[3, 3, 3, 3, 3, 3, 3],
+                num_heads_stripe=[3, 3, 3, 3, 3, 3, 3],
+                window_size=16,
+                stripe_size=[48, 96],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=True,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 16)
+            pad_size = 96
+        case 2:
+            model_name = "db_motion_grl_base_gopro.ckpt"
+            model_args = dict(
+                embed_dim=180,
+                upscale=1,
+                upsampler="",
+                depths=[4, 4, 8, 8, 8, 4, 4],
+                num_heads_window=[3, 3, 3, 3, 3, 3, 3],
+                num_heads_stripe=[3, 3, 3, 3, 3, 3, 3],
+                window_size=12,
+                stripe_size=[48, 96],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=True,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 12)
+            pad_size = 96
+        case 3:
+            model_name = "db_motion_grl_base_realblur_j.ckpt"
+            model_args = dict(
+                embed_dim=180,
+                upscale=1,
+                upsampler="",
+                depths=[4, 4, 8, 8, 8, 4, 4],
+                num_heads_window=[3, 3, 3, 3, 3, 3, 3],
+                num_heads_stripe=[3, 3, 3, 3, 3, 3, 3],
+                window_size=12,
+                stripe_size=[48, 96],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=True,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 12)
+            pad_size = 96
+        case 4:
+            model_name = "db_motion_grl_base_realblur_r.ckpt"
+            model_args = dict(
+                embed_dim=180,
+                upscale=1,
+                upsampler="",
+                depths=[4, 4, 8, 8, 8, 4, 4],
+                num_heads_window=[3, 3, 3, 3, 3, 3, 3],
+                num_heads_stripe=[3, 3, 3, 3, 3, 3, 3],
+                window_size=12,
+                stripe_size=[48, 96],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=True,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 12)
+            pad_size = 96
+        case 5:
+            model_name = "dm_grl_small.ckpt"
+            model_args = dict(
+                embed_dim=128,
+                upscale=1,
+                upsampler="",
+                depths=[4, 4, 4, 4],
+                num_heads_window=[2, 2, 2, 2],
+                num_heads_stripe=[2, 2, 2, 2],
+                window_size=8,
+                stripe_size=[32, 32],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=False,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 8)
+            pad_size = 32
+        case 6:
+            model_name = "dn_grl_small_c3s15.ckpt"
+            model_args = dict(
+                embed_dim=128,
+                upscale=1,
+                upsampler="",
+                depths=[4, 4, 4, 4],
+                num_heads_window=[2, 2, 2, 2],
+                num_heads_stripe=[2, 2, 2, 2],
+                window_size=16,
+                stripe_size=[64, 128],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=False,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 16)
+            pad_size = 128
+        case 7:
+            model_name = "dn_grl_small_c3s25.ckpt"
+            model_args = dict(
+                embed_dim=128,
+                upscale=1,
+                upsampler="",
+                depths=[4, 4, 4, 4],
+                num_heads_window=[2, 2, 2, 2],
+                num_heads_stripe=[2, 2, 2, 2],
+                window_size=16,
+                stripe_size=[64, 128],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=False,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 16)
+            pad_size = 128
+        case 8:
+            model_name = "dn_grl_small_c3s50.ckpt"
+            model_args = dict(
+                embed_dim=128,
+                upscale=1,
+                upsampler="",
+                depths=[4, 4, 4, 4],
+                num_heads_window=[2, 2, 2, 2],
+                num_heads_stripe=[2, 2, 2, 2],
+                window_size=16,
+                stripe_size=[64, 128],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=False,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 16)
+            pad_size = 128
+        case 9:
+            model_name = "jpeg_grl_small_c3q10.ckpt"
+            model_args = dict(
+                embed_dim=128,
+                upscale=1,
+                upsampler="",
+                depths=[4, 4, 4, 4],
+                num_heads_window=[2, 2, 2, 2],
+                num_heads_stripe=[2, 2, 2, 2],
+                window_size=36,
+                stripe_size=[72, 144],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=False,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 36)
+            pad_size = 144
+        case 10:
+            model_name = "jpeg_grl_small_c3q20.ckpt"
+            model_args = dict(
+                embed_dim=128,
+                upscale=1,
+                upsampler="",
+                depths=[4, 4, 4, 4],
+                num_heads_window=[2, 2, 2, 2],
+                num_heads_stripe=[2, 2, 2, 2],
+                window_size=36,
+                stripe_size=[72, 144],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=False,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 36)
+            pad_size = 144
+        case 11:
+            model_name = "jpeg_grl_small_c3q30.ckpt"
+            model_args = dict(
+                embed_dim=128,
+                upscale=1,
+                upsampler="",
+                depths=[4, 4, 4, 4],
+                num_heads_window=[2, 2, 2, 2],
+                num_heads_stripe=[2, 2, 2, 2],
+                window_size=36,
+                stripe_size=[72, 144],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=False,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 36)
+            pad_size = 144
+        case 12:
+            model_name = "jpeg_grl_small_c3q40.ckpt"
+            model_args = dict(
+                embed_dim=128,
+                upscale=1,
+                upsampler="",
+                depths=[4, 4, 4, 4],
+                num_heads_window=[2, 2, 2, 2],
+                num_heads_stripe=[2, 2, 2, 2],
+                window_size=36,
+                stripe_size=[72, 144],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=False,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 36)
+            pad_size = 144
+        case 13:
+            model_name = "sr_grl_small_c3x2.ckpt"
+            model_args = dict(
+                embed_dim=128,
+                upscale=2,
+                upsampler="pixelshuffle",
+                depths=[4, 4, 4, 4],
+                num_heads_window=[2, 2, 2, 2],
+                num_heads_stripe=[2, 2, 2, 2],
+                window_size=32,
+                stripe_size=[64, 64],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=False,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 32)
+            pad_size = 64
+        case 14:
+            model_name = "sr_grl_small_c3x3.ckpt"
+            model_args = dict(
+                embed_dim=128,
+                upscale=3,
+                upsampler="pixelshuffle",
+                depths=[4, 4, 4, 4],
+                num_heads_window=[2, 2, 2, 2],
+                num_heads_stripe=[2, 2, 2, 2],
+                window_size=32,
+                stripe_size=[64, 64],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=False,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 32)
+            pad_size = 64
+        case 15:
+            model_name = "sr_grl_small_c3x4.ckpt"
+            model_args = dict(
+                embed_dim=128,
+                upscale=4,
+                upsampler="pixelshuffle",
+                depths=[4, 4, 4, 4],
+                num_heads_window=[2, 2, 2, 2],
+                num_heads_stripe=[2, 2, 2, 2],
+                window_size=32,
+                stripe_size=[64, 64],
+                stripe_shift=True,
+                mlp_ratio=2,
+                anchor_window_down_factor=4,
+                local_connection=False,
+                fp16=fp16,
+            )
+            tile_pad = fallback(tile_pad, 32)
+            pad_size = 64
+
+    if tile_w > 0 and tile_h > 0:
+        pad_w = math.ceil(min(tile_w + 2 * tile_pad, clip.width) / pad_size) * pad_size
+        pad_h = math.ceil(min(tile_h + 2 * tile_pad, clip.height) / pad_size) * pad_size
+    else:
+        pad_w = math.ceil(clip.width / pad_size) * pad_size
+        pad_h = math.ceil(clip.height / pad_size) * pad_size
+
+    model_args |= dict(img_size=(pad_h, pad_w))
+
+    state_dict = torch.load(os.path.join(model_dir, model_name), map_location="cpu")
+    if "state_dict" in state_dict:
+        state_dict = state_dict["state_dict"]
+        state_dict = {
+            k.removeprefix("model_g."): v
+            for k, v in state_dict.items()
+            if "model_g." in k and "table_" not in k and "index_" not in k and "mask_" not in k
+        }
+    else:
+        state_dict = {k.removeprefix("model."): v for k, v in state_dict.items() if "model." in k}
 
+    module = GRL(**model_args)
     module.load_state_dict(state_dict)
     module.eval().to(device, memory_format=torch.channels_last)
+    if fp16:
+        module.half()
+
+    if cuda_graphs:
+        graph: list[torch.cuda.CUDAGraph] = []
+        static_input: list[torch.Tensor] = []
+        static_output: list[torch.Tensor] = []
+
+        for i in range(num_streams):
+            static_input.append(
+                torch.zeros((1, 3, pad_h, pad_w), dtype=dtype, device=device).to(memory_format=torch.channels_last)
+            )
+
+            torch.cuda.synchronize(device=device)
+            stream[i].wait_stream(torch.cuda.current_stream(device=device))
+            with torch.cuda.stream(stream[i]):
+                module(static_input[i])
+            torch.cuda.current_stream(device=device).wait_stream(stream[i])
+            torch.cuda.synchronize(device=device)
+
+            graph.append(torch.cuda.CUDAGraph())
+            with torch.cuda.graph(graph[i], stream=stream[i]):
+                static_output.append(module(static_input[i]))
+
+        backend = Backend.CUDAGraphs(graph, static_input, static_output)
+    else:
+        backend = Backend.Eager(module)
 
     index = -1
     index_lock = Lock()
 
     @torch.inference_mode()
     def inference(n: int, f: list[vs.VideoFrame]) -> vs.VideoFrame:
         nonlocal index
@@ -97,21 +471,35 @@
             index = (index + 1) % num_streams
             local_index = index
 
         with stream_lock[local_index], torch.cuda.stream(stream[local_index]):
             img = frame_to_tensor(f[0], device)
 
             if tile_w > 0 and tile_h > 0:
-                output = tile_process(img, scale, tile_w, tile_h, tile_pad, module)
+                output = tile_process(
+                    img, model_args["upscale"], tile_w, tile_h, tile_pad, pad_w, pad_h, backend, local_index
+                )
             else:
-                output = module(img)
+                h, w = img.shape[2:]
+                img = F.pad(img, (0, pad_w - w, 0, pad_h - h), "reflect")
+
+                if cuda_graphs:
+                    static_input[local_index].copy_(img)
+                    graph[local_index].replay()
+                    output = static_output[local_index]
+                else:
+                    output = module(img)
+
+                output = output[:, :, : h * model_args["upscale"], : w * model_args["upscale"]]
 
             return tensor_to_frame(output, f[1].copy())
 
-    new_clip = clip.std.BlankClip(width=clip.width * scale, height=clip.height * scale, keep=True)
+    new_clip = clip.std.BlankClip(
+        width=clip.width * model_args["upscale"], height=clip.height * model_args["upscale"], keep=True
+    )
     return new_clip.std.FrameEval(
         lambda n: new_clip.std.ModifyFrame([clip, new_clip], inference), clip_src=[clip, new_clip]
     )
 
 
 def frame_to_tensor(frame: vs.VideoFrame, device: torch.device) -> torch.Tensor:
     array = np.stack([np.asarray(frame[plane]) for plane in range(frame.format.num_planes)])
@@ -122,15 +510,23 @@
     array = tensor.squeeze(0).detach().cpu().numpy()
     for plane in range(frame.format.num_planes):
         np.copyto(np.asarray(frame[plane]), array[plane, :, :])
     return frame
 
 
 def tile_process(
-    img: torch.Tensor, scale: int, tile_w: int, tile_h: int, tile_pad: int, module: torch.nn.Module
+    img: torch.Tensor,
+    scale: int,
+    tile_w: int,
+    tile_h: int,
+    tile_pad: int,
+    pad_w: int,
+    pad_h: int,
+    backend: Backend.Eager | Backend.CUDAGraphs,
+    index: int,
 ) -> torch.Tensor:
     batch, channel, height, width = img.shape
     output_shape = (batch, channel, height * scale, width * scale)
 
     # start with black image
     output = img.new_zeros(output_shape)
 
@@ -158,16 +554,27 @@
 
             # input tile dimensions
             input_tile_width = input_end_x - input_start_x
             input_tile_height = input_end_y - input_start_y
 
             input_tile = img[:, :, input_start_y_pad:input_end_y_pad, input_start_x_pad:input_end_x_pad]
 
+            h, w = input_tile.shape[2:]
+            mode = "reflect" if pad_w - w < w and pad_h - h < h else "replicate"
+            input_tile = F.pad(input_tile, (0, pad_w - w, 0, pad_h - h), mode)
+
             # process tile
-            output_tile = module(input_tile)
+            if isinstance(backend, Backend.CUDAGraphs):
+                backend.static_input[index].copy_(input_tile)
+                backend.graph[index].replay()
+                output_tile = backend.static_output[index]
+            else:
+                output_tile = backend.module(input_tile)
+
+            output_tile = output_tile[:, :, : h * scale, : w * scale]
 
             # output tile area on total image
             output_start_x = input_start_x * scale
             output_end_x = input_end_x * scale
             output_start_y = input_start_y * scale
             output_end_y = input_end_y * scale
```

## vsgrlir/__main__.py

```diff
@@ -18,8 +18,28 @@
         ) as pbar:
             for chunk in r.iter_content(chunk_size=4096):
                 f.write(chunk)
                 pbar.update(len(chunk))
 
 
 if __name__ == "__main__":
-    download_model("https://github.com/HolyWu/vs-grlir/releases/download/model/bsr_grl_base.ckpt")
+    url = "https://github.com/HolyWu/vs-grlir/releases/download/model/"
+    models = [
+        "bsr_grl_base",
+        "db_defocus_single_pixel_grl_base",
+        "db_motion_grl_base_gopro",
+        "db_motion_grl_base_realblur_j",
+        "db_motion_grl_base_realblur_r",
+        "dm_grl_small",
+        "dn_grl_small_c3s15",
+        "dn_grl_small_c3s25",
+        "dn_grl_small_c3s50",
+        "jpeg_grl_small_c3q10",
+        "jpeg_grl_small_c3q20",
+        "jpeg_grl_small_c3q30",
+        "jpeg_grl_small_c3q40",
+        "sr_grl_small_c3x2",
+        "sr_grl_small_c3x3",
+        "sr_grl_small_c3x4",
+    ]
+    for model in models:
+        download_model(url + model + ".ckpt")
```

## vsgrlir/grl.py

```diff
@@ -80,14 +80,15 @@
         attn_drop=0.0,
         drop_path=0.0,
         norm_layer=nn.LayerNorm,
         pretrained_window_size=[0, 0],
         pretrained_stripe_size=[0, 0],
         conv_type="1conv",
         init_method="",
+        fp16=False,
         args=None,
     ):
         super().__init__()
 
         self.dim = dim
         self.input_resolution = input_resolution
         self.init_method = init_method
@@ -114,20 +115,84 @@
                 drop=drop,
                 attn_drop=attn_drop,
                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                 norm_layer=norm_layer,
                 pretrained_window_size=pretrained_window_size,
                 pretrained_stripe_size=pretrained_stripe_size,
                 res_scale=0.1 if init_method == "r" else 1.0,
+                fp16=fp16,
                 args=args,
             )
             self.blocks.append(block)
 
         self.conv = build_last_conv(conv_type, dim)
 
+    def _apply_half(self, fn):
+        self.blocks.half()
+
+        def compute_should_use_set_data(tensor, tensor_applied):
+            if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):
+                # If the new tensor has compatible tensor type as the existing tensor,
+                # the current behavior is to change the tensor in-place using `.data =`,
+                # and the future behavior is to overwrite the existing tensor. However,
+                # changing the current behavior is a BC-breaking change, and we want it
+                # to happen in future releases. So for now we introduce the
+                # `torch.__future__.get_overwrite_module_params_on_conversion()`
+                # global flag to let the user control whether they want the future
+                # behavior of overwriting the existing tensor or not.
+                return not torch.__future__.get_overwrite_module_params_on_conversion()
+            else:
+                return False
+
+        for key, param in self._parameters.items():
+            if param is None:
+                continue
+            # Tensors stored in modules are graph leaves, and we don't want to
+            # track autograd history of `param_applied`, so we have to use
+            # `with torch.no_grad():`
+            with torch.no_grad():
+                param_applied = fn(param)
+            should_use_set_data = compute_should_use_set_data(param, param_applied)
+            if should_use_set_data:
+                param.data = param_applied
+                out_param = param
+            else:
+                assert isinstance(param, nn.Parameter)
+                assert param.is_leaf
+                out_param = nn.Parameter(param_applied, param.requires_grad)
+                self._parameters[key] = out_param
+
+            if param.grad is not None:
+                with torch.no_grad():
+                    grad_applied = fn(param.grad)
+                should_use_set_data = compute_should_use_set_data(param.grad, grad_applied)
+                if should_use_set_data:
+                    assert out_param.grad is not None
+                    out_param.grad.data = grad_applied
+                else:
+                    assert param.grad.is_leaf
+                    out_param.grad = grad_applied.requires_grad_(param.grad.requires_grad)
+
+        for key, buf in self._buffers.items():
+            if buf is not None:
+                self._buffers[key] = fn(buf)
+
+        return self
+
+    def half(self):
+        r"""Casts all floating point parameters and buffers to ``half`` datatype.
+
+        .. note::
+            This method modifies the module in-place.
+
+        Returns:
+            Module: self
+        """
+        return self._apply_half(lambda t: t.half() if t.is_floating_point() else t)
+
     def _init_weights(self):
         for n, m in self.named_modules():
             if self.init_method == "w":
                 if isinstance(m, (nn.Linear, nn.Conv2d)) and n.find("cpb_mlp") < 0:
                     print("nn.Linear and nn.Conv2d weight initilization")
                     m.weight.data *= 0.1
             elif self.init_method == "l":
@@ -232,14 +297,15 @@
         drop_path_rate=0.1,
         norm_layer=nn.LayerNorm,
         pretrained_window_size=[0, 0],
         pretrained_stripe_size=[0, 0],
         conv_type="1conv",
         init_method="n",  # initialization method of the weight parameters used to train large scale models.
         euclidean_dist=False,
+        fp16=False,
         **kwargs,
     ):
         super(GRL, self).__init__()
         # Process the input arguments
         out_channels = out_channels or in_channels
         self.in_channels = in_channels
         self.out_channels = out_channels
@@ -268,14 +334,15 @@
         self.window_size = to_2tuple(window_size)
         self.shift_size = [w // 2 for w in self.window_size]
         self.stripe_size = stripe_size
         self.stripe_groups = stripe_groups
         self.pretrained_window_size = pretrained_window_size
         self.pretrained_stripe_size = pretrained_stripe_size
         self.anchor_window_down_factor = anchor_window_down_factor
+        self.fp16 = fp16
 
         # Head of the network. First convolution.
         self.conv_first = nn.Conv2d(in_channels, embed_dim, 3, 1, 1)
 
         # Body of the network
         self.norm_start = norm_layer(embed_dim)
         self.pos_drop = nn.Dropout(p=drop_rate)
@@ -287,15 +354,15 @@
             {
                 "out_proj_type": out_proj_type,
                 "local_connection": local_connection,
                 "euclidean_dist": euclidean_dist,
             }
         )
         for k, v in self.set_table_index_mask(self.input_resolution).items():
-            self.register_buffer(k, v)
+            self.register_buffer(k, v, persistent=False)
 
         self.layers = nn.ModuleList()
         for i in range(len(depths)):
             layer = TransformerStage(
                 dim=embed_dim,
                 input_resolution=self.input_resolution,
                 depth=depths[i],
@@ -317,14 +384,15 @@
                     sum(depths[:i]) : sum(depths[: i + 1])
                 ],  # no impact on SR results
                 norm_layer=norm_layer,
                 pretrained_window_size=pretrained_window_size,
                 pretrained_stripe_size=pretrained_stripe_size,
                 conv_type=conv_type,
                 init_method=init_method,
+                fp16=fp16,
                 args=args,
             )
             self.layers.append(layer)
         self.norm_end = norm_layer(embed_dim)
 
         # Tail of the network
         self.conv_after_body = build_last_conv(conv_type, embed_dim)
@@ -361,43 +429,127 @@
             self.conv_last = nn.Conv2d(embed_dim, out_channels, 3, 1, 1)
 
         self.apply(self._init_weights)
         if init_method in ["l", "w"] or init_method.find("t") >= 0:
             for layer in self.layers:
                 layer._init_weights()
 
-    def set_table_index_mask(self, x_size):
+    def _apply_half(self, fn):
+        self.conv_first.half()
+        self.norm_start.half()
+        self.pos_drop.half()
+        for layer in self.layers:
+            layer.half()
+        self.conv_after_body.half()
+
+        if self.upsampler == "pixelshuffle":
+            self.conv_before_upsample.half()
+            self.upsample.half()
+            self.conv_last.half()
+        elif self.upsampler == "pixelshuffledirect":
+            self.upsample.half()
+        elif self.upsampler == "nearest+conv":
+            self.conv_before_upsample.half()
+            self.conv_up1.half()
+            self.conv_up2.half()
+            self.conv_hr.half()
+            self.conv_last.half()
+            self.lrelu.half()
+        else:
+            self.conv_last.half()
+
+        def compute_should_use_set_data(tensor, tensor_applied):
+            if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):
+                # If the new tensor has compatible tensor type as the existing tensor,
+                # the current behavior is to change the tensor in-place using `.data =`,
+                # and the future behavior is to overwrite the existing tensor. However,
+                # changing the current behavior is a BC-breaking change, and we want it
+                # to happen in future releases. So for now we introduce the
+                # `torch.__future__.get_overwrite_module_params_on_conversion()`
+                # global flag to let the user control whether they want the future
+                # behavior of overwriting the existing tensor or not.
+                return not torch.__future__.get_overwrite_module_params_on_conversion()
+            else:
+                return False
+
+        for key, param in self._parameters.items():
+            if param is None:
+                continue
+            # Tensors stored in modules are graph leaves, and we don't want to
+            # track autograd history of `param_applied`, so we have to use
+            # `with torch.no_grad():`
+            with torch.no_grad():
+                param_applied = fn(param)
+            should_use_set_data = compute_should_use_set_data(param, param_applied)
+            if should_use_set_data:
+                param.data = param_applied
+                out_param = param
+            else:
+                assert isinstance(param, nn.Parameter)
+                assert param.is_leaf
+                out_param = nn.Parameter(param_applied, param.requires_grad)
+                self._parameters[key] = out_param
+
+            if param.grad is not None:
+                with torch.no_grad():
+                    grad_applied = fn(param.grad)
+                should_use_set_data = compute_should_use_set_data(param.grad, grad_applied)
+                if should_use_set_data:
+                    assert out_param.grad is not None
+                    out_param.grad.data = grad_applied
+                else:
+                    assert param.grad.is_leaf
+                    out_param.grad = grad_applied.requires_grad_(param.grad.requires_grad)
+
+        for key, buf in self._buffers.items():
+            if buf is not None:
+                self._buffers[key] = fn(buf)
+
+        return self
+
+    def half(self):
+        r"""Casts all floating point parameters and buffers to ``half`` datatype.
+
+        .. note::
+            This method modifies the module in-place.
+
+        Returns:
+            Module: self
+        """
+        return self._apply_half(lambda t: t.half() if t.is_floating_point() else t)
+
+    def set_table_index_mask(self, x_size, device=None):
         """
         Two used cases:
         1) At initialization: set the shared buffers.
         2) During forward pass: get the new buffers if the resolution of the input changes
         """
         # ss - stripe_size, sss - stripe_shift_size
         ss, sss = _get_stripe_info(self.stripe_size, self.stripe_groups, True, x_size)
         df = self.anchor_window_down_factor
 
         table_w = get_relative_coords_table_all(
-            self.window_size, self.pretrained_window_size
+            self.window_size, self.pretrained_window_size, device=device
         )
-        table_sh = get_relative_coords_table_all(ss, self.pretrained_stripe_size, df)
+        table_sh = get_relative_coords_table_all(ss, self.pretrained_stripe_size, df, device=device)
         table_sv = get_relative_coords_table_all(
-            ss[::-1], self.pretrained_stripe_size, df
+            ss[::-1], self.pretrained_stripe_size, df, device=device
         )
 
-        index_w = get_relative_position_index_simple(self.window_size)
-        index_sh_a2w = get_relative_position_index_simple(ss, df, False)
-        index_sh_w2a = get_relative_position_index_simple(ss, df, True)
-        index_sv_a2w = get_relative_position_index_simple(ss[::-1], df, False)
-        index_sv_w2a = get_relative_position_index_simple(ss[::-1], df, True)
-
-        mask_w = calculate_mask(x_size, self.window_size, self.shift_size)
-        mask_sh_a2w = calculate_mask_all(x_size, ss, sss, df, False)
-        mask_sh_w2a = calculate_mask_all(x_size, ss, sss, df, True)
-        mask_sv_a2w = calculate_mask_all(x_size, ss[::-1], sss[::-1], df, False)
-        mask_sv_w2a = calculate_mask_all(x_size, ss[::-1], sss[::-1], df, True)
+        index_w = get_relative_position_index_simple(self.window_size, device=device)
+        index_sh_a2w = get_relative_position_index_simple(ss, df, False, device=device)
+        index_sh_w2a = get_relative_position_index_simple(ss, df, True, device=device)
+        index_sv_a2w = get_relative_position_index_simple(ss[::-1], df, False, device=device)
+        index_sv_w2a = get_relative_position_index_simple(ss[::-1], df, True, device=device)
+
+        mask_w = calculate_mask(x_size, self.window_size, self.shift_size, device=device)
+        mask_sh_a2w = calculate_mask_all(x_size, ss, sss, df, False, device=device)
+        mask_sh_w2a = calculate_mask_all(x_size, ss, sss, df, True, device=device)
+        mask_sv_a2w = calculate_mask_all(x_size, ss[::-1], sss[::-1], df, False, device=device)
+        mask_sv_w2a = calculate_mask_all(x_size, ss[::-1], sss[::-1], df, True, device=device)
         return {
             "table_w": table_w,
             "table_sh": table_sh,
             "table_sv": table_sv,
             "index_w": index_w,
             "index_sh_a2w": index_sh_a2w,
             "index_sh_w2a": index_sh_w2a,
@@ -425,17 +577,19 @@
                 "mask_w": self.mask_w,
                 "mask_sh_a2w": self.mask_sh_a2w,
                 "mask_sh_w2a": self.mask_sh_w2a,
                 "mask_sv_a2w": self.mask_sv_a2w,
                 "mask_sv_w2a": self.mask_sv_w2a,
             }
         else:
-            table_index_mask = self.set_table_index_mask(input_resolution)
-            for k, v in table_index_mask.items():
-                table_index_mask[k] = v.to(device)
+            table_index_mask = self.set_table_index_mask(input_resolution, device=device)
+            if self.fp16:
+                for k, v in table_index_mask.items():
+                    if v.is_floating_point():
+                        table_index_mask[k] = v.half()
             return table_index_mask
 
     def _init_weights(self, m):
         if isinstance(m, nn.Linear):
             # Only used to initialize linear layers
             # weight_shape = m.weight.shape
             # if weight_shape[0] > 256 and weight_shape[1] > 256:
@@ -473,26 +627,26 @@
     def forward_features(self, x):
         x_size = (x.shape[2], x.shape[3])
         x = bchw_to_blc(x)
         x = self.norm_start(x)
         x = self.pos_drop(x)
 
         table_index_mask = self.get_table_index_mask(x.device, x_size)
+        x = x.float()
         for layer in self.layers:
             x = layer(x, x_size, table_index_mask)
 
         x = self.norm_end(x)  # B L C
+        if self.fp16:
+            x = x.half()
         x = blc_to_bchw(x, x_size)
 
         return x
 
     def forward(self, x):
-        H, W = x.shape[2:]
-        x = self.check_image_size(x)
-
         x = (x - self.mean) * self.img_range
 
         if self.upsampler == "pixelshuffle":
             # for classical SR
             x = self.conv_first(x)
             x = self.conv_after_body(self.forward_features(x)) + x
             x = self.conv_before_upsample(x)
@@ -525,15 +679,15 @@
             if self.in_channels == self.out_channels:
                 x = x + self.conv_last(res)
             else:
                 x = self.conv_last(res)
 
         x = x / self.img_range + self.mean
 
-        return x[:, :, : H * self.upscale, : W * self.upscale]
+        return x
 
     def flops(self):
         pass
 
     def convert_checkpoint(self, state_dict):
         for k in list(state_dict.keys()):
             if (
```

## vsgrlir/mixed_attn_block.py

```diff
@@ -92,18 +92,19 @@
             table = self.relative_coords_table
             index = self.relative_position_index
         else:
             table = get_relative_coords_table_all(
                 self.window_size,
                 self.pretrained_window_size,
                 self.anchor_window_down_factor,
-            ).to(device)
+                device=device,
+            )
             index = get_relative_position_index_simple(
-                self.window_size, self.anchor_window_down_factor
-            ).to(device)
+                self.window_size, self.anchor_window_down_factor, device=device
+            )
 
         bias_table = self.cpb_mlp(table)  # 2*Wh-1, 2*Ww-1, num_heads
         bias_table = bias_table.view(-1, self.num_heads)
 
         win_dim = prod(self.window_size)
         bias = bias_table[index.view(-1)]
         bias = bias.view(win_dim, win_dim, -1).permute(2, 0, 1).contiguous()
@@ -112,20 +113,18 @@
         attn = attn + bias.unsqueeze(0)
 
         # W-MSA/SW-MSA
         if self.use_buffer:
             mask = self.attn_mask
             # during test and window shift, recalculate the mask
             if self.input_resolution != x_size and self.shift_size > 0:
-                mask = calculate_mask(x_size, self.window_size, self.shift_size)
-                mask = mask.to(attn.device)
+                mask = calculate_mask(x_size, self.window_size, self.shift_size, device=attn.device)
         else:
             if self.shift_size > 0:
-                mask = calculate_mask(x_size, self.window_size, self.shift_size)
-                mask = mask.to(attn.device)
+                mask = calculate_mask(x_size, self.window_size, self.shift_size, device=attn.device)
             else:
                 mask = None
 
         # shift attention mask
         if mask is not None:
             nW = mask.shape[0]
             mask = mask.unsqueeze(1).unsqueeze(0)
@@ -209,20 +208,19 @@
             and not fixed_stripe_size
         ):
             # during test and stripe size is not fixed.
             pretrained_stripe_size = (
                 self.pretrained_stripe_size
             )  # or stripe_size; Needs further pondering
             table = get_relative_coords_table_all(
-                stripe_size, pretrained_stripe_size, self.anchor_window_down_factor
+                stripe_size, pretrained_stripe_size, self.anchor_window_down_factor, device=device
             )
-            table = table.to(device)
             index = get_relative_position_index_simple(
-                stripe_size, self.anchor_window_down_factor, self.window_to_anchor
-            ).to(device)
+                stripe_size, self.anchor_window_down_factor, self.window_to_anchor, device=device
+            )
         else:
             table = self.relative_coords_table
             index = self.relative_position_index
         # The same table size-> 1, Wh+AWh-1, Ww+AWw-1, 2
         # But different index size -> # Wh*Ww, AWh*AWw
         # if N1 < N2:
         #     index = index.transpose(0, 1)
@@ -244,26 +242,26 @@
             if self.input_resolution != x_size and self.stripe_shift > 0:
                 mask = calculate_mask_all(
                     x_size,
                     stripe_size,
                     shift_size,
                     self.anchor_window_down_factor,
                     self.window_to_anchor,
+                    device=device,
                 )
-                mask = mask.to(device)
         else:
             if self.stripe_shift > 0:
                 mask = calculate_mask_all(
                     x_size,
                     stripe_size,
                     shift_size,
                     self.anchor_window_down_factor,
                     self.window_to_anchor,
+                    device=attn.device,
                 )
-                mask = mask.to(attn.device)
             else:
                 mask = None
 
         # shift attention mask
         if mask is not None:
             nW = mask.shape[0]
             mask = mask.unsqueeze(1).unsqueeze(0)
```

## vsgrlir/mixed_attn_block_efficient.py

```diff
@@ -439,14 +439,15 @@
         attn_drop=0.0,
         drop_path=0.0,
         act_layer=nn.GELU,
         norm_layer=nn.LayerNorm,
         pretrained_window_size=[0, 0],
         pretrained_stripe_size=[0, 0],
         res_scale=1.0,
+        fp16=False,
         args=None,
     ):
         super().__init__()
         self.dim = dim
         self.input_resolution = input_resolution
         self.num_heads_w = num_heads_w
         self.num_heads_s = num_heads_s
@@ -459,14 +460,15 @@
             self.stripe_size = stripe_size[::-1]
             self.stripe_groups = stripe_groups[::-1]
         else:
             self.stripe_size = stripe_size
             self.stripe_groups = stripe_groups
         self.mlp_ratio = mlp_ratio
         self.res_scale = res_scale
+        self.fp16 = fp16
 
         self.attn = MixedAttention(
             dim,
             input_resolution,
             num_heads_w,
             num_heads_s,
             window_size,
@@ -531,23 +533,23 @@
     def forward(self, x, x_size, all_table_index_mask):
         # Mixed attention
         table_index_mask = self._get_table_index_mask(all_table_index_mask)
         if self.args.local_connection:
             x = (
                 x
                 + self.res_scale
-                * self.drop_path(self.norm1(self.attn(x, x_size, table_index_mask)))
-                + self.conv(x, x_size)
+                * self.drop_path(self.norm1(self.attn(x.half() if self.fp16 else x, x_size, table_index_mask)))
+                + self.conv(x.half() if self.fp16 else x, x_size)
             )
         else:
             x = x + self.res_scale * self.drop_path(
-                self.norm1(self.attn(x, x_size, table_index_mask))
+                self.norm1(self.attn(x.half() if self.fp16 else x, x_size, table_index_mask))
             )
         # FFN
-        x = x + self.res_scale * self.drop_path(self.norm2(self.mlp(x)))
+        x = x + self.res_scale * self.drop_path(self.norm2(self.mlp(x.half() if self.fp16 else x)))
 
         return x
 
     def extra_repr(self) -> str:
         return (
             f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads=({self.num_heads_w}, {self.num_heads_s}), "
             f"window_size={self.window_size}, window_shift={self.window_shift}, "
```

## vsgrlir/ops.py

```diff
@@ -69,19 +69,19 @@
     x = windows.view(
         B, H // window_size[0], W // window_size[1], window_size[0], window_size[1], -1
     )
     x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
     return x
 
 
-def _fill_window(input_resolution, window_size, shift_size=None):
+def _fill_window(input_resolution, window_size, shift_size=None, device=None):
     if shift_size is None:
         shift_size = [s // 2 for s in window_size]
 
-    img_mask = torch.zeros((1, *input_resolution, 1))  # 1 H W 1
+    img_mask = torch.zeros((1, *input_resolution, 1), device=device)  # 1 H W 1
     h_slices = (
         slice(0, -window_size[0]),
         slice(-window_size[0], -shift_size[0]),
         slice(-shift_size[0], None),
     )
     w_slices = (
         slice(0, -window_size[1]),
@@ -105,22 +105,22 @@
 # 1) Swin Transformer, SwinIR, Square window attention in GRL;
 # 2) Early development of the decomposition-based efficient attention mechanism (efficient_win_attn.py);
 # 3) GRL. Window-anchor attention mechanism.
 # 1) & 3) are still useful
 #####################################
 
 
-def calculate_mask(input_resolution, window_size, shift_size):
+def calculate_mask(input_resolution, window_size, shift_size, device=None):
     """
     Use case: 1)
     """
     # calculate attention mask for SW-MSA
     if isinstance(shift_size, int):
         shift_size = to_2tuple(shift_size)
-    mask_windows = _fill_window(input_resolution, window_size, shift_size)
+    mask_windows = _fill_window(input_resolution, window_size, shift_size, device=device)
 
     attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
     attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(
         attn_mask == 0, float(0.0)
     )  # nW, window_size**2, window_size**2
 
     return attn_mask
@@ -128,27 +128,28 @@
 
 def calculate_mask_all(
     input_resolution,
     window_size,
     shift_size,
     anchor_window_down_factor=1,
     window_to_anchor=True,
+    device=None,
 ):
     """
     Use case: 3)
     """
     # calculate attention mask for SW-MSA
     anchor_resolution = [s // anchor_window_down_factor for s in input_resolution]
     aws = [s // anchor_window_down_factor for s in window_size]
     anchor_shift = [s // anchor_window_down_factor for s in shift_size]
 
     # mask of window1: nW, Wh**Ww
-    mask_windows = _fill_window(input_resolution, window_size, shift_size)
+    mask_windows = _fill_window(input_resolution, window_size, shift_size, device=device)
     # mask of window2: nW, AWh*AWw
-    mask_anchor = _fill_window(anchor_resolution, aws, anchor_shift)
+    mask_anchor = _fill_window(anchor_resolution, aws, anchor_shift, device=device)
 
     if window_to_anchor:
         attn_mask = mask_windows.unsqueeze(2) - mask_anchor.unsqueeze(1)
     else:
         attn_mask = mask_anchor.unsqueeze(2) - mask_windows.unsqueeze(1)
     attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(
         attn_mask == 0, float(0.0)
@@ -174,17 +175,17 @@
     attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(
         attn_mask == 0, float(0.0)
     )  # nW, Wh**Ww, AWh*AWw
 
     return attn_mask
 
 
-def _get_meshgrid_coords(start_coords, end_coords):
-    coord_h = torch.arange(start_coords[0], end_coords[0])
-    coord_w = torch.arange(start_coords[1], end_coords[1])
+def _get_meshgrid_coords(start_coords, end_coords, device=None):
+    coord_h = torch.arange(start_coords[0], end_coords[0], device=device)
+    coord_w = torch.arange(start_coords[1], end_coords[1], device=device)
     coords = torch.stack(torch.meshgrid([coord_h, coord_w], indexing="ij"))  # 2, Wh, Ww
     coords = torch.flatten(coords, 1)  # 2, Wh*Ww
     return coords
 
 
 def get_relative_coords_table(
     window_size, pretrained_window_size=[0, 0], anchor_window_down_factor=1
@@ -219,15 +220,15 @@
         table[:, :, :, 1] /= ts[1] - 1
     table *= 8  # normalize to -8, 8
     table = torch.sign(table) * torch.log2(torch.abs(table) + 1.0) / np.log2(8)
     return table
 
 
 def get_relative_coords_table_all(
-    window_size, pretrained_window_size=[0, 0], anchor_window_down_factor=1
+    window_size, pretrained_window_size=[0, 0], anchor_window_down_factor=1, device=None
 ):
     """
     Use case: 3)
 
     Support all window shapes.
     Args:
         window_size:
@@ -249,16 +250,16 @@
     ts_n = [-(w2 - 1) - (w1 - w2) // 2 for w1, w2 in zip(ws, aws)]
     pts = [w1 - 1 - (w1 - w2) // 2 for w1, w2 in zip(pws, paws)]
 
     # TODO: pretrained window size and pretrained anchor window size is only used here.
     # TODO: Investigate whether it is really important to use this setting when finetuning large window size
     # TODO: based on pretrained weights with small window size.
 
-    coord_h = torch.arange(ts_n[0], ts_p[0] + 1, dtype=torch.float32)
-    coord_w = torch.arange(ts_n[1], ts_p[1] + 1, dtype=torch.float32)
+    coord_h = torch.arange(ts_n[0], ts_p[0] + 1, dtype=torch.float32, device=device)
+    coord_w = torch.arange(ts_n[1], ts_p[1] + 1, dtype=torch.float32, device=device)
     table = torch.stack(torch.meshgrid([coord_h, coord_w], indexing="ij")).permute(
         1, 2, 0
     )
     table = table.contiguous().unsqueeze(0)  # 1, Wh+AWh-1, Ww+AWw-1, 2
     if pts[0] > 0:
         table[:, :, :, 0] /= pts[0]
         table[:, :, :, 1] /= pts[1]
@@ -346,27 +347,27 @@
     else:
         offset = [w1 - s - 1 for s, w1 in zip(coords_anchor_start, ws)]
         idx = coords_diff_odd(coords_anchor, coords, offset, max_horizontal_diff)
     return idx  # Wh*Ww, AWh*AWw or AWh*AWw, Wh*Ww
 
 
 def get_relative_position_index_simple(
-    window_size, anchor_window_down_factor=1, window_to_anchor=True
+    window_size, anchor_window_down_factor=1, window_to_anchor=True, device=None
 ):
     """
     Use case: 3)
     This is a simplified version of get_relative_position_index_all
     The start coordinate of anchor window is also (0, 0)
     get pair-wise relative position index for each token inside the window
     """
     ws = window_size
     aws = [w // anchor_window_down_factor for w in window_size]
 
-    coords = _get_meshgrid_coords((0, 0), window_size)  # 2, Wh*Ww
-    coords_anchor = _get_meshgrid_coords((0, 0), aws)
+    coords = _get_meshgrid_coords((0, 0), window_size, device=device)  # 2, Wh*Ww
+    coords_anchor = _get_meshgrid_coords((0, 0), aws, device=device)
     # 2, AWh*AWw
 
     max_horizontal_diff = aws[1] + ws[1] - 1
     if window_to_anchor:
         offset = [w2 - 1 for w2 in aws]
         idx = coords_diff_odd(coords, coords_anchor, offset, max_horizontal_diff)
     else:
```

## vsgrlir/swin_v1_block.py

```diff
@@ -188,16 +188,15 @@
         x = window_partition(x, self.window_size)  # nW*B, wh, ww, C
         x = x.view(-1, prod(self.window_size), C)  # nW*B, wh*ww, C
 
         # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size
         if self.input_resolution == x_size:
             attn_mask = self.attn_mask
         else:
-            attn_mask = calculate_mask(x_size, self.window_size, self.shift_size)
-            attn_mask = attn_mask.to(x.device)
+            attn_mask = calculate_mask(x_size, self.window_size, self.shift_size, device=x.device)
 
         # attention
         x = super(WindowAttentionWrapperV1, self).forward(x, mask=attn_mask)
         # nW*B, wh*ww, C
 
         # merge windows
         x = x.view(-1, *self.window_size, C)
```

## Comparing `vsgrlir-1.0.0.dist-info/METADATA` & `vsgrlir-1.1.0.dist-info/METADATA`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: vsgrlir
-Version: 1.0.0
+Version: 1.1.0
 Summary: GRLIR function for VapourSynth
 Project-URL: Homepage, https://github.com/HolyWu/vs-grlir
 Project-URL: Bug Tracker, https://github.com/HolyWu/vs-grlir/issues
 Author-email: HolyWu <holywu@gmail.com>
 License: MIT License
         
         Copyright (c) 2023 HolyWu
@@ -36,21 +36,20 @@
 Requires-Dist: numpy>=1.24.2
 Requires-Dist: omegaconf>=2.3.0
 Requires-Dist: requests>=2.28.2
 Requires-Dist: timm>=0.6.13
 Requires-Dist: torch>=1.13.1
 Requires-Dist: tqdm>=4.65.0
 Requires-Dist: vapoursynth>=55
+Requires-Dist: vsutil>=0.8.0
 Description-Content-Type: text/markdown
 
-# SwinIR
+# GRL for Image Restoration
 Efficient and Explicit Modelling of Image Hierarchies for Image Restoration, based on https://github.com/ofsoundof/GRL-Image-Restoration.
 
-Only real-world image super-resolution model is kept.
-
 
 ## Dependencies
 - [NumPy](https://numpy.org/install)
 - [PyTorch](https://pytorch.org/get-started) 1.13.1
 - [VapourSynth](http://www.vapoursynth.com/) R55+
```

## Comparing `vsgrlir-1.0.0.dist-info/licenses/LICENSE` & `vsgrlir-1.1.0.dist-info/licenses/LICENSE`

 * *Files identical despite different names*

